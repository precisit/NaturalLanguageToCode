{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "D:\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n  warnings.warn('`Sequential.model` is deprecated. '\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "\r 32/756 [>.............................] - ETA: 7s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 64/756 [=>............................] - ETA: 6s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 96/756 [==>...........................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r128/756 [====>.........................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r160/756 [=====>........................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r192/756 [======>.......................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r224/756 [=======>......................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/756 [=========>....................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r288/756 [==========>...................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r320/756 [===========>..................] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r352/756 [============>.................] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/756 [==============>...............] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r416/756 [===============>..............] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r448/756 [================>.............] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r480/756 [==================>...........] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r512/756 [===================>..........] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r544/756 [====================>.........] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r576/756 [=====================>........] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r608/756 [=======================>......] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r640/756 [========================>.....] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r672/756 [=========================>....] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r704/756 [==========================>...] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r736/756 [============================>.] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r756/756 [==============================] - 7s 9ms/step\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from basicTrainingData import y_test, X_test, tags, prey_train, preX_train, dict_vectorizer, add_basic_features, y_train, y_val, simple_train_sentences\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import pickle \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from os import system, listdir, path\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "#Deep learning model\n",
    "training_model = load_model('cnn.h5')\n",
    "y_pred = training_model.model.predict(X_test, verbose=1).argmax(-1)\n",
    "y_true = y_test.argmax(-1)\n",
    "\n",
    "#Encoding model\n",
    "pkl_file = open('Departure_encoder.pkl', 'rb')\n",
    "encoder_model = pickle.load(pkl_file) \n",
    "pkl_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "-\nOriginal input:  [('In', 'IN'), ('July', 'NNP'), (',', ','), ('the', 'DT'), ('Environmental', 'NNP'), ('Protection', 'NNP'), ('Agency', 'NNP'), ('imposed', 'VBD'), ('a', 'DT'), ('gradual', 'JJ'), ('ban', 'NN'), ('on', 'IN'), ('virtually', 'RB'), ('all', 'DT'), ('uses', 'NNS'), ('of', 'IN'), ('asbestos', 'NN'), ('.', '.')]\nInput sentence: ['In', 'July', ',', 'the', 'Environmental', 'Protection', 'Agency', 'imposed', 'a', 'gradual', 'ban', 'on', 'virtually', 'all', 'uses', 'of', 'asbestos', '.']\nDecoded sentence: ['IN', 'NNP', ',', 'DT', 'NNP', 'NNP', 'NNP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'RB', 'DT', 'NNS', 'IN', 'NN', '.']\n-\nOriginal input:  [('By', 'IN'), ('1997', 'CD'), (',', ','), ('almost', 'RB'), ('all', 'DT'), ('remaining', 'VBG'), ('uses', 'NNS'), ('of', 'IN'), ('cancer-causing', 'JJ'), ('asbestos', 'NN'), ('will', 'MD'), ('be', 'VB'), ('outlawed', 'VBN'), ('*-6', '-NONE-'), ('.', '.')]\nInput sentence: ['By', '1997', ',', 'almost', 'all', 'remaining', 'uses', 'of', 'cancer-causing', 'asbestos', 'will', 'be', 'outlawed', '*-6', '.']\nDecoded sentence: ['IN', 'CD', ',', 'RB', 'DT', 'VBG', 'NNS', 'IN', 'JJ', 'NN', 'MD', 'VB', 'VBN', '-NONE-', '.']\n-\nOriginal input:  [('About', 'IN'), ('160', 'CD'), ('workers', 'NNS'), ('at', 'IN'), ('a', 'DT'), ('factory', 'NN'), ('that', 'WDT'), ('*T*-8', '-NONE-'), ('made', 'VBD'), ('paper', 'NN'), ('for', 'IN'), ('the', 'DT'), ('Kent', 'NNP'), ('filters', 'NNS'), ('were', 'VBD'), ('exposed', 'VBN'), ('*-7', '-NONE-'), ('to', 'TO'), ('asbestos', 'NN'), ('in', 'IN'), ('the', 'DT'), ('1950s', 'CD'), ('.', '.')]\nInput sentence: ['About', '160', 'workers', 'at', 'a', 'factory', 'that', '*T*-8', 'made', 'paper', 'for', 'the', 'Kent', 'filters', 'were', 'exposed', '*-7', 'to', 'asbestos', 'in', 'the', '1950s', '.']\nDecoded sentence: ['IN', 'CD', 'NNS', 'IN', 'DT', 'NN', 'WDT', '-NONE-', 'VBD', 'NN', 'IN', 'DT', 'NNP', 'NNS', 'VBD', 'VBN', '-NONE-', 'TO', 'NN', 'IN', 'DT', 'CD', '.']\n-\nOriginal input:  [('Areas', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('factory', 'NN'), ('*ICH*-2', '-NONE-'), ('were', 'VBD'), ('particularly', 'RB'), ('dusty', 'JJ'), ('where', 'WRB'), ('the', 'DT'), ('crocidolite', 'NN'), ('was', 'VBD'), ('used', 'VBN'), ('*-8', '-NONE-'), ('*T*-1', '-NONE-'), ('.', '.')]\nInput sentence: ['Areas', 'of', 'the', 'factory', '*ICH*-2', 'were', 'particularly', 'dusty', 'where', 'the', 'crocidolite', 'was', 'used', '*-8', '*T*-1', '.']\nDecoded sentence: ['NNS', 'IN', 'DT', 'NN', '-NONE-', 'VBD', 'RB', 'JJ', 'WRB', 'DT', 'NN', 'VBD', 'VBN', '-NONE-', '-NONE-', '.']\n-\nOriginal input:  [('Workers', 'NNS'), ('dumped', 'VBD'), ('large', 'JJ'), ('burlap', 'NN'), ('sacks', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('imported', 'VBN'), ('material', 'NN'), ('into', 'IN'), ('a', 'DT'), ('huge', 'JJ'), ('bin', 'NN'), (',', ','), ('poured', 'VBD'), ('in', 'RP'), ('cotton', 'NN'), ('and', 'CC'), ('acetate', 'NN'), ('fibers', 'NNS'), ('and', 'CC'), ('mechanically', 'RB'), ('mixed', 'VBD'), ('the', 'DT'), ('dry', 'JJ'), ('fibers', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('process', 'NN'), ('used', 'VBN'), ('*', '-NONE-'), ('*', '-NONE-'), ('to', 'TO'), ('make', 'VB'), ('filters', 'NNS'), ('.', '.')]\nInput sentence: ['Workers', 'dumped', 'large', 'burlap', 'sacks', 'of', 'the', 'imported', 'material', 'into', 'a', 'huge', 'bin', ',', 'poured', 'in', 'cotton', 'and', 'acetate', 'fibers', 'and', 'mechanically', 'mixed', 'the', 'dry', 'fibers', 'in', 'a', 'process', 'used', '*', '*', 'to', 'make', 'filters', '.']\nDecoded sentence: ['NNS', 'VBD', 'JJ', 'NN', 'NNS', 'IN', 'DT', 'VBN', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'VBD', 'RP', 'NN', 'CC', 'NN', 'NNS', 'CC', 'RB', 'VBD', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'VBN', '-NONE-', '-NONE-', 'TO', 'VB', 'NNS', '.']\n-\nOriginal input:  [('Workers', 'NNS'), ('described', 'VBD'), ('``', '``'), ('clouds', 'NNS'), ('of', 'IN'), ('blue', 'JJ'), ('dust', 'NN'), (\"''\", \"''\"), ('that', 'WDT'), ('*T*-1', '-NONE-'), ('hung', 'VBD'), ('over', 'IN'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('factory', 'NN'), (',', ','), ('even', 'RB'), ('though', 'IN'), ('exhaust', 'NN'), ('fans', 'NNS'), ('ventilated', 'VBD'), ('the', 'DT'), ('area', 'NN'), ('.', '.')]\nInput sentence: ['Workers', 'described', '``', 'clouds', 'of', 'blue', 'dust', \"''\", 'that', '*T*-1', 'hung', 'over', 'parts', 'of', 'the', 'factory', ',', 'even', 'though', 'exhaust', 'fans', 'ventilated', 'the', 'area', '.']\nDecoded sentence: ['NNS', 'VBD', '``', 'NNS', 'IN', 'JJ', 'NN', \"''\", 'WDT', '-NONE-', 'VBD', 'IN', 'NNS', 'IN', 'DT', 'NN', ',', 'RB', 'IN', 'NN', 'NNS', 'VBD', 'DT', 'NN', '.']\n-\nOriginal input:  [('``', '``'), ('There', 'EX'), (\"'s\", 'VBZ'), ('no', 'DT'), ('question', 'NN'), ('that', 'IN'), ('some', 'DT'), ('of', 'IN'), ('those', 'DT'), ('workers', 'NNS'), ('and', 'CC'), ('managers', 'NNS'), ('contracted', 'VBD'), ('asbestos-related', 'JJ'), ('diseases', 'NNS'), (',', ','), (\"''\", \"''\"), ('said', 'VBD'), ('*T*-1', '-NONE-'), ('Darrell', 'NNP'), ('Phillips', 'NNP'), (',', ','), ('vice', 'NN'), ('president', 'NN'), ('of', 'IN'), ('human', 'JJ'), ('resources', 'NNS'), ('for', 'IN'), ('Hollingsworth', 'NNP'), ('&', 'CC'), ('Vose', 'NNP'), ('.', '.')]\nInput sentence: ['``', 'There', \"'s\", 'no', 'question', 'that', 'some', 'of', 'those', 'workers', 'and', 'managers', 'contracted', 'asbestos-related', 'diseases', ',', \"''\", 'said', '*T*-1', 'Darrell', 'Phillips', ',', 'vice', 'president', 'of', 'human', 'resources', 'for', 'Hollingsworth', '&', 'Vose', '.']\nDecoded sentence: ['``', 'EX', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'IN', 'DT', 'NNS', 'CC', 'NNS', 'VBD', 'JJ', 'NNS', ',', \"''\", 'VBD', '-NONE-', 'NNP', 'NNP', ',', 'NN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'NNP', 'CC', 'NNP', '.']\n-\nOriginal input:  [('``', '``'), ('But', 'CC'), ('you', 'PRP'), ('have', 'VBP'), ('*-1', '-NONE-'), ('to', 'TO'), ('recognize', 'VB'), ('that', 'IN'), ('these', 'DT'), ('events', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('35', 'CD'), ('years', 'NNS'), ('ago', 'IN'), ('.', '.')]\nInput sentence: ['``', 'But', 'you', 'have', '*-1', 'to', 'recognize', 'that', 'these', 'events', 'took', 'place', '35', 'years', 'ago', '.']\nDecoded sentence: ['``', 'CC', 'PRP', 'VBP', '-NONE-', 'TO', 'VB', 'IN', 'DT', 'NNS', 'VBD', 'NN', 'CD', 'NNS', 'IN', '.']\n-\nOriginal input:  [('It', 'PRP'), ('has', 'VBZ'), ('no', 'DT'), ('bearing', 'NN'), ('on', 'IN'), ('our', 'PRP$'), ('work', 'NN'), ('force', 'NN'), ('today', 'NN'), ('.', '.')]\nInput sentence: ['It', 'has', 'no', 'bearing', 'on', 'our', 'work', 'force', 'today', '.']\nDecoded sentence: ['PRP', 'VBZ', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'NN', 'NN', '.']\n-\nOriginal input:  [('Yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ('mutual', 'JJ'), ('funds', 'NNS'), ('continued', 'VBD'), ('*-1', '-NONE-'), ('to', 'TO'), ('slide', 'VB'), (',', ','), ('amid', 'IN'), ('signs', 'NNS'), ('that', 'IN'), ('portfolio', 'NN'), ('managers', 'NNS'), ('expect', 'VBP'), ('further', 'JJ'), ('declines', 'NNS'), ('in', 'IN'), ('interest', 'NN'), ('rates', 'NNS'), ('.', '.')]\nInput sentence: ['Yields', 'on', 'money-market', 'mutual', 'funds', 'continued', '*-1', 'to', 'slide', ',', 'amid', 'signs', 'that', 'portfolio', 'managers', 'expect', 'further', 'declines', 'in', 'interest', 'rates', '.']\nDecoded sentence: ['NNS', 'IN', 'JJ', 'JJ', 'NNS', 'VBD', '-NONE-', 'TO', 'VB', ',', 'IN', 'NNS', 'IN', 'NN', 'NNS', 'VBP', 'JJ', 'NNS', 'IN', 'NN', 'NNS', '.']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "counter = 0\n",
    "for seq_index in range(10):\n",
    "  print('-')\n",
    "  print('Original input: ', preX_train[seq_index])\n",
    "  print('Input sentence:', simple_train_sentences[seq_index])\n",
    "  print('Decoded sentence:', prey_train[counter:counter+len(simple_train_sentences[seq_index])])\n",
    "  counter += len(simple_train_sentences[seq_index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def clean_up_example(text):\n",
    "    cleaned = re.sub('\\W+', ' ', text)\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    return lemmatized\n",
    "\n",
    "#These properties could include informations about\n",
    "# previous and next words as well as prefixes and suffixes.\n",
    "def add_basic_features(sentence_terms, index):\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'nb_terms': len(sentence_terms),\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_terms) - 1,\n",
    "        'is_capitalized': term[0].upper() == term[0],\n",
    "        'is_all_caps': term.upper() == term,\n",
    "        'is_all_lower': term.lower() == term,\n",
    "        'prefix-1': term[0],\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'suffix-1': term[-1],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'next_word': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1]\n",
    "    }\n",
    "\n",
    "\n",
    "def give_tag(probabilities):\n",
    "    #Take out index number of most probable in each list, then decodes them    \n",
    "    maximum = probabilities.max()\n",
    "    index_of_maximum = np.where(probabilities == maximum)\n",
    "    tag = encoder_model.inverse_transform(index_of_maximum[0])\n",
    "    return tag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "-\nInput word: walk\nDecoded tag: ['NN']\n-\nInput word: downn\nDecoded tag: ['NN']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "example = clean_up_example(\"walk downn\")\n",
    "lem_list = []\n",
    "\n",
    "for i in range(len(example)): \n",
    "    lem_list.append(add_basic_features(example, i))\n",
    "\n",
    "example_result = training_model.predict(dict_vectorizer.transform(lem_list))\n",
    "\n",
    "for i in range(len(lem_list)):\n",
    "  print('-')\n",
    "  print('Input word:', example[i])\n",
    "  print('Decoded tag:',give_tag(example_result[i]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "system(\"cls\")\n",
    "system(\"color b9\")\n",
    "FILE = open(\"data.txt\", \"r\")\n",
    "listOFCommands = FILE.readlines()\n",
    "post_data = {}\n",
    "post_data['command'] = []\n",
    "\n",
    "for command in listOFCommands:\n",
    "    command_list = clean_up_example(command)\n",
    "    temp_list = []\n",
    "    temp_tags = []\n",
    "    for i in range(len(command_list)): \n",
    "        temp_list.append(add_basic_features(command, i))\n",
    "    tagged_command = training_model.predict(dict_vectorizer.transform(temp_list))\n",
    "    for i in range(len(command_list)):\n",
    "        temp_tags.append(give_tag(tagged_command[i])[0])\n",
    "    post_data['command'].append({'sentence' : command, 'tags' : temp_tags})\n",
    "        \n",
    "data_path = 'data_commands.txt'\n",
    "with open(data_path, 'w') as outfile:\n",
    "    json.dump(post_data, outfile)\n",
    "\n",
    "FILE.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_model = load_model('cnn.h5')\n",
    "pkl_file = open('Departure_encoder.pkl', 'rb')\n",
    "encoder_model = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "pkl_file = open('dict_vectorizer.pkl', 'rb')\n",
    "dict_vectorizer = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos() == \"n\"])\n",
    "    pos_counts[\"v\"] = len([item for item in probable_part_of_speech if item.pos() == \"v\"])\n",
    "    pos_counts[\"a\"] = len([item for item in probable_part_of_speech if item.pos() == \"a\"])\n",
    "    pos_counts[\"r\"] = len([item for item in probable_part_of_speech if item.pos() == \"r\"])\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "\n",
    "def clean_up_example(text):\n",
    "    cleaned = re.sub('\\W+', ' ', text)\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    return lemmatized\n",
    "\n",
    "#These properties could include informations about\n",
    "# previous and next words as well as prefixes and suffixes.\n",
    "def add_basic_features(sentence_terms, index):\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'nb_terms': len(sentence_terms),\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_terms) - 1,\n",
    "        'is_capitalized': term[0].upper() == term[0],\n",
    "        'is_all_caps': term.upper() == term,\n",
    "        'is_all_lower': term.lower() == term,\n",
    "        'prefix-1': term[0],\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'suffix-1': term[-1],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'next_word': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1]\n",
    "    }\n",
    "\n",
    "def give_tag(probabilities):\n",
    "    #Take out index number of most probable in each list, then decodes them\n",
    "    maximum = probabilities.max()\n",
    "    index_of_maximum = np.where(probabilities == maximum)\n",
    "    tag = encoder_model.inverse_transform(index_of_maximum[0])\n",
    "    return tag\n",
    "\n",
    "example = clean_up_example(\"walk down\")\n",
    "lem_list = []\n",
    "\n",
    "for i in range(len(example)):\n",
    "    lem_list.append(add_basic_features(example, i))\n",
    "\n",
    "example_result = training_model.predict(dict_vectorizer.transform(lem_list))\n",
    "\n",
    "for i in range(len(lem_list)):\n",
    "    print('-')\n",
    "    print('Input word:', example[i])\n",
    "    print('Decoded tag:', give_tag(example_result[i]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "-\nInput word: walk\nDecoded tag: ['NN']\n-\nInput word: down\nDecoded tag: ['NN']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "training_model = load_model('cnn.h5')\n",
    "pkl_file = open('Departure_encoder.pkl', 'rb')\n",
    "encoder_model = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "pkl_file = open('dict_vectorizer.pkl', 'rb')\n",
    "dict_vectorizer = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos() == \"n\"])\n",
    "    pos_counts[\"v\"] = len([item for item in probable_part_of_speech if item.pos() == \"v\"])\n",
    "    pos_counts[\"a\"] = len([item for item in probable_part_of_speech if item.pos() == \"a\"])\n",
    "    pos_counts[\"r\"] = len([item for item in probable_part_of_speech if item.pos() == \"r\"])\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "\n",
    "def clean_up_example(text):\n",
    "    cleaned = re.sub('\\W+', ' ', text)\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    return lemmatized\n",
    "\n",
    "#These properties could include informations about\n",
    "# previous and next words as well as prefixes and suffixes.\n",
    "def add_basic_features(sentence_terms, index):\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'nb_terms': len(sentence_terms),\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_terms) - 1,\n",
    "        'is_capitalized': term[0].upper() == term[0],\n",
    "        'is_all_caps': term.upper() == term,\n",
    "        'is_all_lower': term.lower() == term,\n",
    "        'prefix-1': term[0],\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'suffix-1': term[-1],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'next_word': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1]\n",
    "    }\n",
    "\n",
    "def give_tag(probabilities):\n",
    "    #Take out index number of most probable in each list, then decodes them\n",
    "    maximum = probabilities.max()\n",
    "    index_of_maximum = np.where(probabilities == maximum)\n",
    "    tag = encoder_model.inverse_transform(index_of_maximum[0])\n",
    "    return tag\n",
    "\n",
    "example = clean_up_example(\"walk down\")\n",
    "lem_list = []\n",
    "\n",
    "for i in range(len(example)):\n",
    "    lem_list.append(add_basic_features(example, i))\n",
    "\n",
    "example_result = training_model.predict(dict_vectorizer.transform(lem_list))\n",
    "\n",
    "for i in range(len(lem_list)):\n",
    "    print('-')\n",
    "    print('Input word:', example[i])\n",
    "    print('Decoded tag:', give_tag(example_result[i]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}