{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from basicTrainingData import y_test, X_test, tags, prey_train, preX_train, dict_vectorizer, add_basic_features, y_train, y_val, simple_train_sentences\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import pickle \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Deep learning model\n",
    "training_model = load_model('keras_mlp.h5')\n",
    "y_pred = training_model.model.predict(X_test, verbose=1).argmax(-1)\n",
    "y_true = y_test.argmax(-1)\n",
    "\n",
    "#Encoding model\n",
    "pkl_file = open('Departure_encoder.pkl', 'rb')\n",
    "encoder_model = pickle.load(pkl_file) \n",
    "pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for seq_index in range(10):\n",
    "  print('-')\n",
    "  print('Original input: ', preX_train[seq_index])\n",
    "  print('Input sentence:', simple_train_sentences[seq_index])\n",
    "  print('Decoded sentence:', prey_train[counter:counter+len(simple_train_sentences[seq_index])])\n",
    "  counter += len(simple_train_sentences[seq_index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_up_example(text):\n",
    "    cleaned = re.sub('\\W+', ' ', text)\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    return lemmatized\n",
    "\n",
    "#These properties could include informations about\n",
    "# previous and next words as well as prefixes and suffixes.\n",
    "def add_basic_features(sentence_terms, index):\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'nb_terms': len(sentence_terms),\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_terms) - 1,\n",
    "        'is_capitalized': term[0].upper() == term[0],\n",
    "        'is_all_caps': term.upper() == term,\n",
    "        'is_all_lower': term.lower() == term,\n",
    "        'prefix-1': term[0],\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'suffix-1': term[-1],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'next_word': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1]\n",
    "    }\n",
    "\n",
    "\n",
    "def give_tag(probabilities):\n",
    "    #Take out index number of most probable in each list, then decodes them    \n",
    "    maximum = probabilities.max()\n",
    "    index_of_maximum = np.where(probabilities == maximum)\n",
    "    tag = encoder_model.inverse_transform(index_of_maximum[0])\n",
    "    return tag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#What to keep?\n",
    "import numpy as np\n",
    "\n",
    "example = clean_up_example(\"walk down\")\n",
    "lem_list = []\n",
    "\n",
    "for i in range(len(example)): \n",
    "    lem_list.append(add_basic_features(example, i))\n",
    "\n",
    "example_result = training_model.predict(dict_vectorizer.transform(lem_list))\n",
    "\n",
    "for i in range(len(lem_list)):\n",
    "  print('-')\n",
    "  print('Input sentence:', example[i])\n",
    "  print('Decoded sentence:',give_tag(example_result[i]))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}